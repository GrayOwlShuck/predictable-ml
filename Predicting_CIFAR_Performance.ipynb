{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: scaling of performance with data\n",
    "\n",
    "For reference, the original paper is [here](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "\n",
    "For the first experiment, we're just going to subsample the data and see how peformance changes as we increase the training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import altair\n",
    "import scipy\n",
    "\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dataset_size(start = 0.5, end = 100, base=2):\n",
    "    \"\"\" Returns exponentially distributed dataset size vector\"\"\"\n",
    "    dataset_size=[start]\n",
    "    while True:\n",
    "        dataset_size.append(dataset_size[-1]*base)\n",
    "        if dataset_size[-1] > end:\n",
    "            dataset_size[-1] = end\n",
    "            break\n",
    "    \n",
    "    return dataset_size\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our simple ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hyperparam_dict=None):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        if not hyperparam_dict :\n",
    "            hyperparam_dict = self.standard_hyperparams()\n",
    "        \n",
    "        self.hyperparam_dict = hyperparam_dict\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, hyperparam_dict['conv1_size'], 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(hyperparam_dict['conv1_size'], hyperparam_dict['conv2_size'], 5)\n",
    "        self.fc1 = nn.Linear(hyperparam_dict['conv2_size'] * 5 * 5, hyperparam_dict['fc1_size'])\n",
    "        self.fc2 = nn.Linear(hyperparam_dict['fc1_size'], hyperparam_dict['fc2_size'])\n",
    "        self.fc3 = nn.Linear(hyperparam_dict['fc2_size'], 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        print(x.size())\n",
    "        x = x.view(-1, self.hyperparam_dict['conv2_size'] * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def standard_hyperparams(self):\n",
    "        hyperparam_dict = {}\n",
    "        \n",
    "        hyperparam_dict['conv1_size'] = 6\n",
    "        hyperparam_dict['conv2_size'] = 16\n",
    "        \n",
    "        hyperparam_dict['fc1_size'] = 120\n",
    "        hyperparam_dict['fc2_size'] = 84\n",
    "        \n",
    "        return hyperparam_dict\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, hyperparam_dict=None):\n",
    "        \n",
    "        super(DeeperNet, self).__init__()\n",
    "        \n",
    "        if not hyperparam_dict :\n",
    "            hyperparam_dict = self.standard_hyperparams()\n",
    "        \n",
    "        self.hyperparam_dict = hyperparam_dict\n",
    "        self.pool = nn.MaxPool2d(2, 2)        \n",
    "        self.conv1 = nn.Conv2d(3, hyperparam_dict['conv1_size'], 5)\n",
    "        self.conv2 = nn.Conv2d(hyperparam_dict['conv1_size'], hyperparam_dict['conv2_size'], 5)\n",
    "        self.conv3 = nn.Conv2d(hyperparam_dict['conv2_size'], hyperparam_dict['conv3_size'], 2)\n",
    "        self.conv4 = nn.Conv2d(hyperparam_dict['conv3_size'], hyperparam_dict['conv4_size'], 2)\n",
    "        self.conv5 = nn.Conv2d(hyperparam_dict['conv4_size'], hyperparam_dict['conv5_size'], 2)\n",
    "        self.conv6 = nn.Conv2d(hyperparam_dict['conv5_size'], hyperparam_dict['conv6_size'], 2)\n",
    "        self.fc1 = nn.Linear(hyperparam_dict['conv6_size'], hyperparam_dict['fc1_size'])\n",
    "        self.fc2 = nn.Linear(hyperparam_dict['fc1_size'], hyperparam_dict['fc2_size'])\n",
    "        self.fc3 = nn.Linear(hyperparam_dict['fc2_size'], 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = x.view(-1, self.hyperparam_dict['conv6_size'])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def standard_hyperparams(self):\n",
    "        hyperparam_dict = {}\n",
    "        \n",
    "        hyperparam_dict['conv1_size'] = 6\n",
    "        hyperparam_dict['conv2_size'] = 16\n",
    "        hyperparam_dict['conv3_size'] = 32\n",
    "        hyperparam_dict['conv4_size'] = 32\n",
    "        hyperparam_dict['conv5_size'] = 64\n",
    "        hyperparam_dict['conv6_size'] = 128\n",
    "        \n",
    "        hyperparam_dict['fc1_size'] = 120\n",
    "        hyperparam_dict['fc2_size'] = 84\n",
    "        \n",
    "        return hyperparam_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(net, trainloader, valloader, n_val, n_epochs=10, lr=0.001, momentum=0.9, weight_decay=0.1):\n",
    "    \"\"\" Take a model, run some number of training steps on some data.\n",
    "    \"\"\"\n",
    "    print('Inside training, model ID', id(net))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay) \n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    best_val_loss=10\n",
    "    for epoch in range(n_epochs):  \n",
    "        if epoch > 4 and (val_loss_list[-1] > val_loss_list[-2]) and (val_loss_list[-2] > val_loss_list[-3]) and (val_loss_list[-3] > val_loss_list[-4]):\n",
    "            break\n",
    "        epoch_start = time.time()\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        for i_train, data in enumerate(trainloader, 0):\n",
    "            \n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "                \n",
    "\n",
    "        eval_start = time.time()\n",
    "        \n",
    "        for i_val, data in enumerate(valloader, 0):\n",
    "        \n",
    "            if i_val> n_val:\n",
    "                    break\n",
    "\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # get output\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            val_loss += loss.data[0]\n",
    "\n",
    "            \n",
    "        train_loss_list.append(running_loss/i_train)\n",
    "        val_loss_list.append(val_loss/i_val)\n",
    "        print(\"Epoch %d done, avg_train_loss=%1.2f, avg_val_loss=%1.2f, batches=%d \\n Took %1.2f minutes for training, %1.2f for eval.\"\n",
    "              %(epoch, running_loss/i_train, val_loss/i_val, i_train, (eval_start-epoch_start)/60, (time.time() - eval_start)/60))\n",
    "        \n",
    "        if val_loss/i_val < best_val_loss:\n",
    "            print('Found better model')\n",
    "            saved_net = net\n",
    "            best_val_loss = val_loss/i_val\n",
    "            \n",
    "    print('Finished Training')\n",
    "    \n",
    "    # Return the net with the best loss\n",
    "    print('Returning net with %1.3f loss'%best_val_loss)\n",
    "    return saved_net, train_loss_list, val_loss_list\n",
    "\n",
    "def test_model(net, testloader):\n",
    "    \"\"\" Test a model on a dataset.\"\"\"\n",
    "    \n",
    "    print('Test UID is', id(net))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss=0\n",
    "    for data in testloader:\n",
    "        inputs, orig_labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(orig_labels)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == orig_labels).sum()\n",
    "        total_loss += loss.data[0]\n",
    "    \n",
    "    return correct/total, total_loss/total\n",
    "\n",
    "def test_saved_model(dataset_size):\n",
    "    \"\"\" Load a trained model and test it for a list of dataset sizes.\"\"\"\n",
    "\n",
    "    test_acc = {}\n",
    "    val_acc = {}\n",
    "    train_acc = {}\n",
    "    test_loss = {}\n",
    "    for train_size in dataset_size:\n",
    "        net = torch.load('trainset_%1.2f_%d_images.model'%(train_size, train_size*total_train))\n",
    "        print('Loaded model with subset %1.4f, which is %d images'%(train_size, train_size*total_train))\n",
    "\n",
    "        accuracy, loss = test_model(net, testloader)\n",
    "\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "            100 * accuracy))\n",
    "\n",
    "        test_acc[train_size] = accuracy\n",
    "        test_loss[train_size] = loss\n",
    "\n",
    "        del net # Attempt to free-up memory!\n",
    "        \n",
    "    return test_acc, test_loss\n",
    "\n",
    "def test_best_saved_model(dataset_size, regexp='trainset_%d_images*.model'):\n",
    "    \"\"\" Retrieve the saved model with the best validation score.\"\"\"\n",
    "\n",
    "    test_acc = {}\n",
    "    val_acc = {}\n",
    "    train_acc = {}\n",
    "    test_loss = {}\n",
    "\n",
    "    for train_size in dataset_size:\n",
    "        \n",
    "        candidates = glob.glob(regexp%(train_size*total_train))\n",
    "\n",
    "        best_model = sorted(candidates, key = lambda  str: re.sub(\".*loss\", \"\", str))[0]\n",
    "        \n",
    "        net = torch.load(best_model)\n",
    "        print('Loaded model %s' %(best_model))\n",
    "\n",
    "        accuracy, loss = test_model(net, testloader)\n",
    "\n",
    "        print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "            100 * accuracy))\n",
    "\n",
    "        test_acc[train_size] = accuracy\n",
    "        test_loss[train_size] = loss\n",
    "\n",
    "        del net # Attempt to free-up memory!\n",
    "        \n",
    "    return test_acc, test_loss\n",
    "\n",
    "def plot_results(test_acc, test_loss):\n",
    "    \"\"\" Plot dataset size vs accuracy and loss\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(dataset_size*total_train, list(test_acc.values()), color='g', linewidth=2, marker='x', markersize=8)\n",
    "    plt.xlabel('Training Examples', fontsize=20)\n",
    "    plt.ylabel('Accuracy', fontsize=20)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = plt.loglog()\n",
    "    plt.plot(dataset_size*total_train, list(test_acc.values()), color='g', linewidth=2, marker='x', markersize=8)\n",
    "    plt.xlabel('log(Training Examples)', fontsize=20)\n",
    "    plt.ylabel('log Accuracy', fontsize=20)\n",
    "    plt.suptitle('Dataset size vs. Accuracy', fontsize=25)\n",
    "\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(dataset_size*total_train, list(test_loss.values()), color='r', linewidth=2, marker='x', markersize=8)\n",
    "    plt.xlabel('Training Examples', fontsize=18)\n",
    "    plt.ylabel('Loss', fontsize=18)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = plt.loglog()\n",
    "    plt.plot(dataset_size*total_train, list(test_loss.values()), color='r', linewidth=2, marker='x', markersize=8)\n",
    "    plt.xlabel('log(Training Examples)', fontsize=18)\n",
    "    plt.ylabel('log Loss', fontsize=18)\n",
    "    plt.suptitle('Dataset size vs. Loss', fontsize=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(val_size * num_train))\n",
    "\n",
    "np.random.seed()\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "total_train = len(train_idx)\n",
    "\n",
    "# For each of our train sets, we want a subset of the true train set\n",
    "dataset_size = np.array(get_dataset_size())\n",
    "dataset_size /=100 # Convert to fraction of original dataset size\n",
    "\n",
    "\n",
    "trainset_samplers = dict()\n",
    "trainset_loaders = dict()\n",
    "for ts in dataset_size:\n",
    "    trainset_samplers[ts]=np.random.choice(train_idx, int(ts*total_train))\n",
    "    trainset_loaders[ts]=torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          sampler=trainset_samplers[ts], num_workers=2)\n",
    "    \n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "valloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          sampler=val_sampler, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005 0.01  0.02  0.04  0.08  0.16  0.32  0.64  1.   ]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset 0.0050, which is 200 images\n",
      "UID is 4788317488\n",
      "Inside training, model ID 4788317488\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.00 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.00 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.00 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.00 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=49 \n",
      " Took 0.00 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "Test UID is 4788317488\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Training with subset 0.0100, which is 400 images\n",
      "UID is 4551266376\n",
      "Inside training, model ID 4551266376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archy/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=99 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=99 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.29, avg_val_loss=2.30, batches=99 \n",
      " Took 0.01 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.27, avg_val_loss=2.27, batches=99 \n",
      " Took 0.01 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.23, avg_val_loss=2.26, batches=99 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.19, avg_val_loss=2.24, batches=99 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.13, avg_val_loss=2.21, batches=99 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.07, avg_val_loss=2.20, batches=99 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.199 loss\n",
      "Test UID is 4551266376\n",
      "Accuracy of the network on the 10000 test images: 18 %\n",
      "Training with subset 0.0200, which is 800 images\n",
      "UID is 4711032368\n",
      "Inside training, model ID 4711032368\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.29, batches=199 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.29, avg_val_loss=2.27, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.24, avg_val_loss=2.22, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.18, avg_val_loss=2.19, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.13, avg_val_loss=2.15, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.08, avg_val_loss=2.11, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.05, avg_val_loss=2.08, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.02, avg_val_loss=2.06, batches=199 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=1.99, avg_val_loss=2.03, batches=199 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.033 loss\n",
      "Test UID is 4711032368\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Training with subset 0.0400, which is 1600 images\n",
      "UID is 4710853656\n",
      "Inside training, model ID 4710853656\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.29, avg_val_loss=2.27, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.21, avg_val_loss=2.13, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.10, avg_val_loss=2.05, batches=399 \n",
      " Took 0.04 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.01, avg_val_loss=1.99, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.95, avg_val_loss=1.93, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=1.89, avg_val_loss=1.88, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=1.84, avg_val_loss=1.85, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=1.79, avg_val_loss=1.82, batches=399 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=1.73, avg_val_loss=1.80, batches=399 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 1.799 loss\n",
      "Test UID is 4710853656\n",
      "Accuracy of the network on the 10000 test images: 34 %\n",
      "Training with subset 0.0800, which is 3200 images\n",
      "UID is 4551266376\n",
      "Inside training, model ID 4551266376\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.28, batches=799 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.22, avg_val_loss=2.17, batches=799 \n",
      " Took 0.07 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.11, avg_val_loss=2.02, batches=799 \n",
      " Took 0.07 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=1.93, avg_val_loss=1.83, batches=799 \n",
      " Took 0.09 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.79, avg_val_loss=1.73, batches=799 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=1.69, avg_val_loss=1.67, batches=799 \n",
      " Took 0.07 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=1.60, avg_val_loss=1.61, batches=799 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=1.51, avg_val_loss=1.63, batches=799 \n",
      " Took 0.07 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=1.42, avg_val_loss=1.69, batches=799 \n",
      " Took 0.07 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.611 loss\n",
      "Test UID is 4551266376\n",
      "Accuracy of the network on the 10000 test images: 41 %\n",
      "Training with subset 0.1600, which is 6400 images\n",
      "UID is 4711032368\n",
      "Inside training, model ID 4711032368\n",
      "Epoch 0 done, avg_train_loss=2.27, avg_val_loss=2.12, batches=1599 \n",
      " Took 0.14 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=1.97, avg_val_loss=2.02, batches=1599 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=1.74, avg_val_loss=1.76, batches=1599 \n",
      " Took 0.14 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=1.60, avg_val_loss=1.66, batches=1599 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=1.50, avg_val_loss=1.60, batches=1599 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.41, avg_val_loss=1.57, batches=1599 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=1.32, avg_val_loss=1.61, batches=1599 \n",
      " Took 0.13 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=1.22, avg_val_loss=1.66, batches=1599 \n",
      " Took 0.16 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=1.13, avg_val_loss=1.71, batches=1599 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.572 loss\n",
      "Test UID is 4711032368\n",
      "Accuracy of the network on the 10000 test images: 45 %\n",
      "Training with subset 0.3200, which is 12800 images\n",
      "UID is 4711032368\n",
      "Inside training, model ID 4711032368\n",
      "Epoch 0 done, avg_train_loss=2.14, avg_val_loss=1.85, batches=3199 \n",
      " Took 0.27 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=1.72, avg_val_loss=1.62, batches=3199 \n",
      " Took 180.31 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=1.51, avg_val_loss=1.53, batches=3199 \n",
      " Took 8559.84 minutes for training, 0.19 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=1.40, avg_val_loss=1.49, batches=3199 \n",
      " Took 0.53 minutes for training, 0.22 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=1.30, avg_val_loss=1.47, batches=3199 \n",
      " Took 1.82 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.21, avg_val_loss=1.48, batches=3199 \n",
      " Took 0.61 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=1.12, avg_val_loss=1.48, batches=3199 \n",
      " Took 1.15 minutes for training, 0.17 for eval.\n",
      "Epoch 7 done, avg_train_loss=1.03, avg_val_loss=1.56, batches=3199 \n",
      " Took 1.20 minutes for training, 0.08 for eval.\n",
      "Epoch 8 done, avg_train_loss=0.94, avg_val_loss=1.59, batches=3199 \n",
      " Took 0.52 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=0.85, avg_val_loss=1.72, batches=3199 \n",
      " Took 0.51 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.469 loss\n",
      "Test UID is 4711032368\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "Training with subset 0.6400, which is 25600 images\n",
      "UID is 4551266376\n",
      "Inside training, model ID 4551266376\n",
      "Epoch 0 done, avg_train_loss=1.88, avg_val_loss=1.59, batches=6399 \n",
      " Took 0.97 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=1.46, avg_val_loss=1.46, batches=6399 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=1.29, avg_val_loss=1.44, batches=6399 \n",
      " Took 0.84 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=1.15, avg_val_loss=1.37, batches=6399 \n",
      " Took 0.78 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=1.03, avg_val_loss=1.44, batches=6399 \n",
      " Took 1.22 minutes for training, 0.12 for eval.\n",
      "Epoch 5 done, avg_train_loss=0.93, avg_val_loss=1.51, batches=6399 \n",
      " Took 1.55 minutes for training, 0.12 for eval.\n",
      "Epoch 6 done, avg_train_loss=0.84, avg_val_loss=1.65, batches=6399 \n",
      " Took 0.79 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.371 loss\n",
      "Test UID is 4551266376\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Training with subset 1.0000, which is 40000 images\n",
      "UID is 4711032368\n",
      "Inside training, model ID 4711032368\n",
      "Epoch 0 done, avg_train_loss=1.77, avg_val_loss=1.48, batches=9999 \n",
      " Took 1.19 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=1.33, avg_val_loss=1.31, batches=9999 \n",
      " Took 1.20 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=1.15, avg_val_loss=1.31, batches=9999 \n",
      " Took 1.02 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=1.02, avg_val_loss=1.29, batches=9999 \n",
      " Took 1.25 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=0.92, avg_val_loss=1.34, batches=9999 \n",
      " Took 0.93 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=0.83, avg_val_loss=1.46, batches=9999 \n",
      " Took 0.90 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=0.75, avg_val_loss=1.58, batches=9999 \n",
      " Took 0.84 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.293 loss\n",
      "Test UID is 4711032368\n",
      "Accuracy of the network on the 10000 test images: 54 %\n"
     ]
    }
   ],
   "source": [
    "test_acc = {}\n",
    "val_acc = {}\n",
    "train_acc = {}\n",
    "test_loss = {}\n",
    "for train_size in dataset_size:\n",
    "    print('Training with subset %1.4f, which is %d images'%(train_size, train_size*total_train))\n",
    "    net = Net()\n",
    "    print('UID is', id(net))\n",
    "    net, loss_list, val_list = train_model(net, trainset_loaders[train_size], valloader, 1000, n_epochs=10,\n",
    "                                          lr=0.001, momentum=0.9, weight_decay=0)\n",
    "    \n",
    "    _test_accuracy, _test_loss = test_model(net, testloader)\n",
    "    \n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * _test_accuracy))\n",
    "    \n",
    "    test_acc[train_size] = _test_accuracy\n",
    "    test_loss[train_size] = _test_loss\n",
    "    val_acc[train_size] = val_list\n",
    "    train_acc[train_size] = loss_list\n",
    "    \n",
    "    torch.save(net, 'trainset_%1.2f_%d_images.model'%(train_size, train_size*total_train))\n",
    "    del net # Attempt to free-up memory!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "As expected, test set performance improves with dataset size.\n",
    "\n",
    "However, when plotted on a log-log scale, this scaling is sub-linear: we get less bang-for-our-buck than one might expect given the increase in dataset size.\n",
    "\n",
    "This is because we're naively using the same hyperparamters, which seem to work better for smaller datasets. In the second experiment we'll rectify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with subset 0.0050, which is 200 images\n",
      "Test UID is 4786004824\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Loaded model with subset 0.0100, which is 400 images\n",
      "Test UID is 4711160184\n",
      "Accuracy of the network on the 10000 test images: 18 %\n",
      "Loaded model with subset 0.0200, which is 800 images\n",
      "Test UID is 4786004488\n",
      "Accuracy of the network on the 10000 test images: 25 %\n",
      "Loaded model with subset 0.0400, which is 1600 images\n",
      "Test UID is 4786007008\n",
      "Accuracy of the network on the 10000 test images: 34 %\n",
      "Loaded model with subset 0.0800, which is 3200 images\n",
      "Test UID is 4711161640\n",
      "Accuracy of the network on the 10000 test images: 41 %\n",
      "Loaded model with subset 0.1600, which is 6400 images\n",
      "Test UID is 4786005384\n",
      "Accuracy of the network on the 10000 test images: 45 %\n",
      "Loaded model with subset 0.3200, which is 12800 images\n",
      "Test UID is 4786004432\n",
      "Accuracy of the network on the 10000 test images: 48 %\n",
      "Loaded model with subset 0.6400, which is 25600 images\n",
      "Test UID is 4711160856\n",
      "Accuracy of the network on the 10000 test images: 54 %\n",
      "Loaded model with subset 1.0000, which is 40000 images\n",
      "Test UID is 4711160968\n",
      "Accuracy of the network on the 10000 test images: 54 %\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test_saved_model(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAHnCAYAAAAB9kTyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQj5P1I3Z33rjZsrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ce9550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4YAAAHmCAYAAAAvGJB+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQgghhBBCyDvuv758hl3mC2KpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11da79a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(test_acc, test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiment 2\n",
    "The best hyperparameter for each model depends critically upon the dataset size. We're going to do a little hyperparamter search using [random search](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_hyperparamters():\n",
    "    \"\"\" Returns a random sampling of hyperparameters.\"\"\"\n",
    "    \n",
    "    hyperparam_dict = {}\n",
    "    \n",
    "    hyperparam_dict['lr'] = 10 ** np.random.uniform(-6, -1)\n",
    "    hyperparam_dict['weight_decay'] = 10 ** np.random.uniform(-6, -3)\n",
    "    hyperparam_dict['momentum'] = 10 ** np.random.uniform(-1, 0)\n",
    "    \n",
    "    hyperparam_dict['conv1_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['conv2_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['fc1_size'] = int(np.random.uniform(30,200))\n",
    "    hyperparam_dict['fc2_size'] = int(np.random.uniform(30,200))\n",
    "\n",
    "    return hyperparam_dict\n",
    "\n",
    "def deeper_random_hyperparamters():\n",
    "    \"\"\" Returns a random sampling of hyperparameters.\"\"\"\n",
    "    \n",
    "    hyperparam_dict = {}\n",
    "    \n",
    "    hyperparam_dict['lr'] = 10 ** np.random.uniform(-6, -1)\n",
    "    hyperparam_dict['weight_decay'] = 10 ** np.random.uniform(-6, -2)\n",
    "    hyperparam_dict['momentum'] = 10 ** np.random.uniform(-1, 0)\n",
    "    \n",
    "    hyperparam_dict['conv1_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['conv2_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['conv3_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['conv4_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['conv5_size'] = int(np.random.uniform(10,100))\n",
    "    hyperparam_dict['conv6_size'] = int(np.random.uniform(10,100))\n",
    "    \n",
    "    hyperparam_dict['fc1_size'] = int(np.random.uniform(30,200))\n",
    "    hyperparam_dict['fc2_size'] = int(np.random.uniform(30,200))\n",
    "\n",
    "    return hyperparam_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search for each dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset 0.0050, which is 200 images\n",
      "{'lr': 1.5508769329851e-05, 'weight_decay': 0.0005467689562194923, 'momentum': 0.3249750314657417, 'conv1_size': 75, 'conv2_size': 42, 'fc1_size': 196, 'fc2_size': 74}\n",
      "Inside training, model ID 4804244256\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n",
      "torch.Size([4, 42, 5, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10424:\n",
      "Process Process-10423:\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/archy/anaconda/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7740d3d97a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                               \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparam_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                                               )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-87dfa61d28be>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, trainloader, valloader, n_val, n_epochs, lr, momentum, weight_decay)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_acc = {}\n",
    "val_acc = {}\n",
    "train_acc = {}\n",
    "test_loss = {}\n",
    "\n",
    "# Testing parameters\n",
    "n_searches = 2\n",
    "n_epochs = 1\n",
    "n_val = 10\n",
    "\n",
    "# Real parameters\n",
    "n_searches = 20\n",
    "n_epochs = 15\n",
    "n_val = 500\n",
    "\n",
    "\n",
    "for train_size in dataset_size:\n",
    "    print('Training with subset %1.4f, which is %d images'%(train_size, train_size*total_train))\n",
    "    \n",
    "    test_acc[train_size] = []\n",
    "    test_loss[train_size] = []\n",
    "    val_acc[train_size] = []\n",
    "    train_acc[train_size] = []\n",
    "    \n",
    "    for trial in range(n_searches):\n",
    "        \n",
    "        hyperparam_dict = random_hyperparamters()\n",
    "        print(hyperparam_dict)\n",
    "        \n",
    "        net = Net(hyperparam_dict)\n",
    "        \n",
    "        net, loss_list, val_list = train_model(net, trainset_loaders[train_size], valloader, n_val, n_epochs=n_epochs,\n",
    "                                              lr=hyperparam_dict['lr'], \n",
    "                                               momentum=hyperparam_dict['momentum'], \n",
    "                                               weight_decay=hyperparam_dict['weight_decay']\n",
    "                                              )\n",
    "\n",
    "        test_acc[train_size].append((hyperparam_dict, accuracy))\n",
    "        test_loss[train_size].append((hyperparam_dict, loss)) \n",
    "        val_acc[train_size].append((hyperparam_dict, val_list)) \n",
    "        train_acc[train_size].append((hyperparam_dict, loss_list))\n",
    "\n",
    "\n",
    "        torch.save(net, 'trainset_%d_images_trial%d_val_loss_%1.2f.model'%((train_size*total_train), trial, val_list[-1]))\n",
    "        torch.save(hyperparam_dict, 'trainset_%d_images_trial%d_val_loss_%1.2f.hparams'%((train_size*total_train), trial, val_list[-1]))\n",
    "        \n",
    "        del net # Attempt to free-up memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best model for each dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model trainset_200_images_trial7_val_loss_2.16.model\n",
      "Accuracy of the network on the 10000 test images: 20 %\n",
      "Loaded model trainset_400_images_trial1_val_loss_1.98.model\n",
      "Accuracy of the network on the 10000 test images: 28 %\n",
      "Loaded model trainset_800_images_trial0_val_loss_1.93.model\n",
      "Accuracy of the network on the 10000 test images: 29 %\n",
      "Loaded model trainset_1600_images_trial2_val_loss_1.61.model\n",
      "Accuracy of the network on the 10000 test images: 43 %\n",
      "Loaded model trainset_3200_images_trial7_val_loss_1.59.model\n",
      "Accuracy of the network on the 10000 test images: 42 %\n",
      "Loaded model trainset_6400_images_trial0_val_loss_1.38.model\n",
      "Accuracy of the network on the 10000 test images: 51 %\n",
      "Loaded model trainset_12800_images_trial0_val_loss_1.38.model\n",
      "Accuracy of the network on the 10000 test images: 60 %\n",
      "Loaded model trainset_25600_images_trial4_val_loss_1.07.model\n",
      "Accuracy of the network on the 10000 test images: 63 %\n",
      "Loaded model trainset_40000_images_trial15_val_loss_1.00.model\n",
      "Accuracy of the network on the 10000 test images: 65 %\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = test_best_saved_model(dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size effects, with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+IAAAHnCAYAAAA8SOfRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQykRFxEREREREQkA/0/94KOhImthFAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7c5739860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+cAAAHmCAYAAAARPf8wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncyGEEEIIIYQQooH9FzOdSxvW0MYGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7cae864e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(test_acc, test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat the process for a deeper net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset 0.0050, which is 200 images\n",
      "{'lr': 0.0002415751408154045, 'weight_decay': 0.00039821993121162555, 'momentum': 0.4394040329100565, 'conv1_size': 53, 'conv2_size': 12, 'conv3_size': 22, 'conv4_size': 95, 'conv5_size': 45, 'conv6_size': 48, 'fc1_size': 78, 'fc2_size': 117}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0005352014666506954, 'weight_decay': 5.409178955050957e-06, 'momentum': 0.4474113488341683, 'conv1_size': 10, 'conv2_size': 47, 'conv3_size': 97, 'conv4_size': 20, 'conv5_size': 14, 'conv6_size': 83, 'fc1_size': 159, 'fc2_size': 138}\n",
      "Inside training, model ID 4804169512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archy/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type DeeperNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.027469524382294046, 'weight_decay': 1.1285108933336589e-05, 'momentum': 0.5294457327847325, 'conv1_size': 94, 'conv2_size': 87, 'conv3_size': 63, 'conv4_size': 72, 'conv5_size': 43, 'conv6_size': 22, 'fc1_size': 145, 'fc2_size': 31}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.310 loss\n",
      "{'lr': 5.415014313241935e-06, 'weight_decay': 0.0024439385586483785, 'momentum': 0.6152879605744784, 'conv1_size': 86, 'conv2_size': 63, 'conv3_size': 99, 'conv4_size': 24, 'conv5_size': 68, 'conv6_size': 41, 'fc1_size': 182, 'fc2_size': 172}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.007450515689717344, 'weight_decay': 0.0001432181100114267, 'momentum': 0.38673977215652794, 'conv1_size': 45, 'conv2_size': 64, 'conv3_size': 88, 'conv4_size': 89, 'conv5_size': 82, 'conv6_size': 58, 'fc1_size': 173, 'fc2_size': 79}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 3.4164671265506146e-05, 'weight_decay': 0.0004216786712222242, 'momentum': 0.17049427055033498, 'conv1_size': 51, 'conv2_size': 28, 'conv3_size': 26, 'conv4_size': 60, 'conv5_size': 33, 'conv6_size': 55, 'fc1_size': 42, 'fc2_size': 184}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.0341536967136927e-06, 'weight_decay': 1.3549155499451564e-05, 'momentum': 0.20170920695601732, 'conv1_size': 85, 'conv2_size': 90, 'conv3_size': 65, 'conv4_size': 98, 'conv5_size': 88, 'conv6_size': 89, 'fc1_size': 193, 'fc2_size': 119}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.001664859986589474, 'weight_decay': 0.0013602461932735925, 'momentum': 0.11989299936222815, 'conv1_size': 42, 'conv2_size': 41, 'conv3_size': 42, 'conv4_size': 96, 'conv5_size': 18, 'conv6_size': 98, 'fc1_size': 199, 'fc2_size': 129}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0018577561074180206, 'weight_decay': 1.1976440255963598e-05, 'momentum': 0.46705874335844305, 'conv1_size': 42, 'conv2_size': 54, 'conv3_size': 78, 'conv4_size': 11, 'conv5_size': 63, 'conv6_size': 40, 'fc1_size': 168, 'fc2_size': 57}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 9.535704130679782e-06, 'weight_decay': 1.9752997712216453e-06, 'momentum': 0.4203945272691925, 'conv1_size': 14, 'conv2_size': 95, 'conv3_size': 65, 'conv4_size': 13, 'conv5_size': 45, 'conv6_size': 13, 'fc1_size': 92, 'fc2_size': 178}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.00027945476639204157, 'weight_decay': 5.180793735844201e-05, 'momentum': 0.5214238256979097, 'conv1_size': 33, 'conv2_size': 10, 'conv3_size': 25, 'conv4_size': 67, 'conv5_size': 86, 'conv6_size': 84, 'fc1_size': 73, 'fc2_size': 189}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.6101050890225018e-05, 'weight_decay': 0.0012059112297538991, 'momentum': 0.9239832683497076, 'conv1_size': 29, 'conv2_size': 83, 'conv3_size': 95, 'conv4_size': 98, 'conv5_size': 43, 'conv6_size': 73, 'fc1_size': 90, 'fc2_size': 65}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.305 loss\n",
      "{'lr': 0.09931565680231634, 'weight_decay': 4.843234233861241e-06, 'momentum': 0.22935109908279977, 'conv1_size': 78, 'conv2_size': 89, 'conv3_size': 86, 'conv4_size': 36, 'conv5_size': 21, 'conv6_size': 29, 'fc1_size': 69, 'fc2_size': 88}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.36, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.33, batches=49 \n",
      " Took 59.92 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.32, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.33, batches=49 \n",
      " Took 0.02 minutes for training, 59.91 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.33, batches=49 \n",
      " Took 0.02 minutes for training, 0.09 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.35, avg_val_loss=2.33, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.33, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.33, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.33, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.33, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.33, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.32, batches=49 \n",
      " Took 0.02 minutes for training, 0.09 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.312 loss\n",
      "{'lr': 0.006605349099860398, 'weight_decay': 0.002356394129325798, 'momentum': 0.10703284857384673, 'conv1_size': 41, 'conv2_size': 75, 'conv3_size': 51, 'conv4_size': 40, 'conv5_size': 86, 'conv6_size': 42, 'fc1_size': 92, 'fc2_size': 183}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.36, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.00020675747612546928, 'weight_decay': 1.670600712688923e-06, 'momentum': 0.4260157220501351, 'conv1_size': 41, 'conv2_size': 69, 'conv3_size': 64, 'conv4_size': 65, 'conv5_size': 58, 'conv6_size': 85, 'fc1_size': 188, 'fc2_size': 59}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.001301393546472643, 'weight_decay': 0.0040366820618656755, 'momentum': 0.8351516302022148, 'conv1_size': 90, 'conv2_size': 54, 'conv3_size': 94, 'conv4_size': 60, 'conv5_size': 43, 'conv6_size': 46, 'fc1_size': 156, 'fc2_size': 121}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.10 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.002074606327719964, 'weight_decay': 1.1793649413129098e-05, 'momentum': 0.2236536936165144, 'conv1_size': 67, 'conv2_size': 88, 'conv3_size': 99, 'conv4_size': 43, 'conv5_size': 41, 'conv6_size': 20, 'fc1_size': 165, 'fc2_size': 93}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0022755148353385394, 'weight_decay': 6.527920170555891e-05, 'momentum': 0.22790130402522027, 'conv1_size': 17, 'conv2_size': 45, 'conv3_size': 46, 'conv4_size': 61, 'conv5_size': 97, 'conv6_size': 15, 'fc1_size': 91, 'fc2_size': 186}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.01 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0011969205626523281, 'weight_decay': 0.0017059686427814578, 'momentum': 0.6746106594208015, 'conv1_size': 80, 'conv2_size': 67, 'conv3_size': 22, 'conv4_size': 18, 'conv5_size': 77, 'conv6_size': 30, 'fc1_size': 174, 'fc2_size': 125}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.35, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.35, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.09 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.02 minutes for training, 0.09 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.02 minutes for training, 0.08 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.09 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.20143730657178e-05, 'weight_decay': 0.00012075419479472023, 'momentum': 0.399206807462376, 'conv1_size': 91, 'conv2_size': 87, 'conv3_size': 85, 'conv4_size': 24, 'conv5_size': 47, 'conv6_size': 47, 'fc1_size': 121, 'fc2_size': 101}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.04 minutes for training, 0.12 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.12 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.04 minutes for training, 0.13 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=49 \n",
      " Took 0.03 minutes for training, 0.11 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "Training with subset 0.0100, which is 400 images\n",
      "{'lr': 0.03177978000906266, 'weight_decay': 0.0005088122941086264, 'momentum': 0.11805798601387818, 'conv1_size': 78, 'conv2_size': 58, 'conv3_size': 60, 'conv4_size': 49, 'conv5_size': 96, 'conv6_size': 82, 'fc1_size': 171, 'fc2_size': 133}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.06 minutes for training, 0.09 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.33, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 7.961644168701113e-06, 'weight_decay': 0.0012089049750295085, 'momentum': 0.2110086970743842, 'conv1_size': 91, 'conv2_size': 48, 'conv3_size': 55, 'conv4_size': 49, 'conv5_size': 23, 'conv6_size': 50, 'fc1_size': 169, 'fc2_size': 124}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.07 minutes for training, 0.12 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.006665050603460563, 'weight_decay': 0.005033929891129921, 'momentum': 0.1902418289032807, 'conv1_size': 55, 'conv2_size': 85, 'conv3_size': 94, 'conv4_size': 53, 'conv5_size': 18, 'conv6_size': 11, 'fc1_size': 140, 'fc2_size': 82}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 3.1662239630589686e-05, 'weight_decay': 0.000534296982596971, 'momentum': 0.25137160478787013, 'conv1_size': 34, 'conv2_size': 97, 'conv3_size': 76, 'conv4_size': 28, 'conv5_size': 76, 'conv6_size': 60, 'fc1_size': 113, 'fc2_size': 197}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.06053891328386707, 'weight_decay': 0.00020218132542493424, 'momentum': 0.8544531176900444, 'conv1_size': 19, 'conv2_size': 35, 'conv3_size': 68, 'conv4_size': 90, 'conv5_size': 34, 'conv6_size': 29, 'fc1_size': 85, 'fc2_size': 87}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.38, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.34, avg_val_loss=2.37, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.34, avg_val_loss=2.38, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.34, avg_val_loss=2.37, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.34, avg_val_loss=2.35, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.34, avg_val_loss=2.37, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.34, avg_val_loss=2.38, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.34, avg_val_loss=2.37, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.34, avg_val_loss=2.36, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.349 loss\n",
      "{'lr': 1.9939082164616714e-05, 'weight_decay': 1.8491745245643507e-06, 'momentum': 0.19799843202341522, 'conv1_size': 91, 'conv2_size': 16, 'conv3_size': 90, 'conv4_size': 14, 'conv5_size': 16, 'conv6_size': 11, 'fc1_size': 76, 'fc2_size': 192}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.10 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 9.775417085377575e-05, 'weight_decay': 5.578215273788348e-06, 'momentum': 0.9669296308775797, 'conv1_size': 83, 'conv2_size': 47, 'conv3_size': 80, 'conv4_size': 88, 'conv5_size': 58, 'conv6_size': 67, 'fc1_size': 65, 'fc2_size': 147}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.009189897650076264, 'weight_decay': 3.727050803731134e-06, 'momentum': 0.8110406230977605, 'conv1_size': 26, 'conv2_size': 88, 'conv3_size': 48, 'conv4_size': 95, 'conv5_size': 73, 'conv6_size': 62, 'fc1_size': 175, 'fc2_size': 175}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.03 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.03197266969341266, 'weight_decay': 0.007191312744797098, 'momentum': 0.12272475789124285, 'conv1_size': 85, 'conv2_size': 99, 'conv3_size': 72, 'conv4_size': 11, 'conv5_size': 17, 'conv6_size': 60, 'fc1_size': 131, 'fc2_size': 87}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.07 minutes for training, 0.12 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.07 minutes for training, 0.11 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.11 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.07 minutes for training, 0.11 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.07 minutes for training, 0.10 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.07 minutes for training, 0.10 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.306 loss\n",
      "{'lr': 0.0004595035000940116, 'weight_decay': 0.0005501532591821753, 'momentum': 0.269699867748069, 'conv1_size': 14, 'conv2_size': 21, 'conv3_size': 14, 'conv4_size': 13, 'conv5_size': 91, 'conv6_size': 53, 'fc1_size': 116, 'fc2_size': 146}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.02 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 7.052797674452237e-06, 'weight_decay': 3.1402920387919724e-06, 'momentum': 0.9972316535515516, 'conv1_size': 37, 'conv2_size': 26, 'conv3_size': 46, 'conv4_size': 89, 'conv5_size': 25, 'conv6_size': 77, 'fc1_size': 190, 'fc2_size': 66}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 7.692752982508496e-05, 'weight_decay': 0.00227284009677098, 'momentum': 0.14400930328500447, 'conv1_size': 97, 'conv2_size': 76, 'conv3_size': 52, 'conv4_size': 80, 'conv5_size': 13, 'conv6_size': 58, 'fc1_size': 191, 'fc2_size': 100}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.14 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.07 minutes for training, 0.13 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.12 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.16 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.09 minutes for training, 0.13 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.15 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.09 minutes for training, 0.15 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.08 minutes for training, 0.14 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.08105316158855214, 'weight_decay': 0.00275668569876523, 'momentum': 0.31556883302362027, 'conv1_size': 10, 'conv2_size': 60, 'conv3_size': 39, 'conv4_size': 22, 'conv5_size': 19, 'conv6_size': 21, 'fc1_size': 104, 'fc2_size': 147}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.32, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.34, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.02 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.33, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.323 loss\n",
      "{'lr': 0.0004953700075978807, 'weight_decay': 4.9854968593650436e-05, 'momentum': 0.17464772877805942, 'conv1_size': 34, 'conv2_size': 61, 'conv3_size': 81, 'conv4_size': 71, 'conv5_size': 31, 'conv6_size': 20, 'fc1_size': 130, 'fc2_size': 165}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.14 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.10 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.10 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.004400506602162968, 'weight_decay': 1.0486271551941177e-06, 'momentum': 0.3894042416453206, 'conv1_size': 93, 'conv2_size': 48, 'conv3_size': 23, 'conv4_size': 20, 'conv5_size': 51, 'conv6_size': 82, 'fc1_size': 182, 'fc2_size': 136}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.07 minutes for training, 0.13 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.07 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.07 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.12 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.06 minutes for training, 0.12 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.07 minutes for training, 0.11 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.0006297026363037022, 'weight_decay': 0.0012969517967559945, 'momentum': 0.6786502596782042, 'conv1_size': 40, 'conv2_size': 96, 'conv3_size': 73, 'conv4_size': 62, 'conv5_size': 64, 'conv6_size': 35, 'fc1_size': 156, 'fc2_size': 134}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.07 minutes for training, 0.11 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.10 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.06 minutes for training, 0.09 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 2.560759861772906e-05, 'weight_decay': 2.481172072287739e-06, 'momentum': 0.16159487330682565, 'conv1_size': 13, 'conv2_size': 59, 'conv3_size': 33, 'conv4_size': 87, 'conv5_size': 75, 'conv6_size': 57, 'fc1_size': 97, 'fc2_size': 61}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.03 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 4.0020925520159575e-05, 'weight_decay': 0.006806040005251575, 'momentum': 0.22234168408354985, 'conv1_size': 50, 'conv2_size': 10, 'conv3_size': 64, 'conv4_size': 57, 'conv5_size': 51, 'conv6_size': 42, 'fc1_size': 47, 'fc2_size': 35}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.03 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0007708946043478115, 'weight_decay': 3.2234925428871785e-06, 'momentum': 0.8058651422919261, 'conv1_size': 41, 'conv2_size': 45, 'conv3_size': 31, 'conv4_size': 61, 'conv5_size': 94, 'conv6_size': 71, 'fc1_size': 65, 'fc2_size': 110}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.05 minutes for training, 0.09 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=99 \n",
      " Took 0.05 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.04 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.005180296715756699, 'weight_decay': 0.0036351726438702088, 'momentum': 0.8464690368853731, 'conv1_size': 99, 'conv2_size': 75, 'conv3_size': 93, 'conv4_size': 86, 'conv5_size': 96, 'conv6_size': 84, 'fc1_size': 103, 'fc2_size': 68}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.30, batches=99 \n",
      " Took 0.09 minutes for training, 0.14 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=99 \n",
      " Took 0.09 minutes for training, 0.13 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.08 minutes for training, 0.14 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=99 \n",
      " Took 0.09 minutes for training, 0.15 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.09 minutes for training, 0.13 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.09 minutes for training, 0.14 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.09 minutes for training, 0.13 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.09 minutes for training, 0.13 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.08 minutes for training, 0.14 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.09 minutes for training, 0.13 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.08 minutes for training, 0.13 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.33, batches=99 \n",
      " Took 0.09 minutes for training, 0.15 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.33, batches=99 \n",
      " Took 0.09 minutes for training, 0.15 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=99 \n",
      " Took 0.09 minutes for training, 0.14 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "Training with subset 0.0200, which is 800 images\n",
      "{'lr': 0.023998885522582062, 'weight_decay': 8.753276164432106e-06, 'momentum': 0.14936358488941281, 'conv1_size': 32, 'conv2_size': 97, 'conv3_size': 83, 'conv4_size': 26, 'conv5_size': 47, 'conv6_size': 56, 'fc1_size': 48, 'fc2_size': 101}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.10 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.306 loss\n",
      "{'lr': 8.338534064499182e-06, 'weight_decay': 2.2135070176234582e-05, 'momentum': 0.3750748033065573, 'conv1_size': 99, 'conv2_size': 82, 'conv3_size': 48, 'conv4_size': 94, 'conv5_size': 21, 'conv6_size': 72, 'fc1_size': 156, 'fc2_size': 97}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.20 minutes for training, 0.16 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.17 minutes for training, 0.15 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.17 minutes for training, 0.15 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.15 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.18 minutes for training, 0.14 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.014273608865629343, 'weight_decay': 5.3712165597641404e-05, 'momentum': 0.4419742767915549, 'conv1_size': 17, 'conv2_size': 10, 'conv3_size': 52, 'conv4_size': 40, 'conv5_size': 28, 'conv6_size': 54, 'fc1_size': 183, 'fc2_size': 161}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.0046896684034410715, 'weight_decay': 9.605483499795908e-05, 'momentum': 0.2472430383784234, 'conv1_size': 24, 'conv2_size': 12, 'conv3_size': 65, 'conv4_size': 98, 'conv5_size': 83, 'conv6_size': 37, 'fc1_size': 165, 'fc2_size': 36}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.05625976962460004, 'weight_decay': 0.00031872727888388125, 'momentum': 0.7851643602848453, 'conv1_size': 44, 'conv2_size': 27, 'conv3_size': 52, 'conv4_size': 99, 'conv5_size': 42, 'conv6_size': 14, 'fc1_size': 56, 'fc2_size': 188}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.33, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.33, avg_val_loss=2.32, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.314 loss\n",
      "{'lr': 4.153413457205207e-05, 'weight_decay': 0.0022993918675311134, 'momentum': 0.36630463204522823, 'conv1_size': 62, 'conv2_size': 73, 'conv3_size': 99, 'conv4_size': 29, 'conv5_size': 61, 'conv6_size': 27, 'fc1_size': 168, 'fc2_size': 182}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.12 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.11 minutes for training, 0.09 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.11 minutes for training, 0.10 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.11 minutes for training, 0.09 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.11 minutes for training, 0.11 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.13 minutes for training, 0.11 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.12 minutes for training, 0.11 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.12 minutes for training, 0.11 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.12 minutes for training, 0.11 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.08 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0001975243183735536, 'weight_decay': 0.009207331344535623, 'momentum': 0.8071728773018795, 'conv1_size': 64, 'conv2_size': 57, 'conv3_size': 62, 'conv4_size': 86, 'conv5_size': 48, 'conv6_size': 51, 'fc1_size': 156, 'fc2_size': 119}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 6.966585909348281e-05, 'weight_decay': 0.007537661204707029, 'momentum': 0.11276101702922638, 'conv1_size': 39, 'conv2_size': 95, 'conv3_size': 22, 'conv4_size': 51, 'conv5_size': 55, 'conv6_size': 17, 'fc1_size': 172, 'fc2_size': 192}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 4.55673267860343e-06, 'weight_decay': 0.0010621448088275003, 'momentum': 0.24555345826346625, 'conv1_size': 87, 'conv2_size': 79, 'conv3_size': 86, 'conv4_size': 32, 'conv5_size': 37, 'conv6_size': 32, 'fc1_size': 181, 'fc2_size': 125}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.09 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.027566155470293875, 'weight_decay': 5.963449571296006e-06, 'momentum': 0.11009351749160869, 'conv1_size': 55, 'conv2_size': 36, 'conv3_size': 62, 'conv4_size': 71, 'conv5_size': 74, 'conv6_size': 23, 'fc1_size': 44, 'fc2_size': 62}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.305 loss\n",
      "{'lr': 0.007565264266753071, 'weight_decay': 0.00011159690179134019, 'momentum': 0.7584524486103381, 'conv1_size': 46, 'conv2_size': 61, 'conv3_size': 23, 'conv4_size': 37, 'conv5_size': 46, 'conv6_size': 19, 'fc1_size': 113, 'fc2_size': 116}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.06848911230452681, 'weight_decay': 0.0004078720364107942, 'momentum': 0.257179501641639, 'conv1_size': 96, 'conv2_size': 89, 'conv3_size': 68, 'conv4_size': 16, 'conv5_size': 79, 'conv6_size': 75, 'fc1_size': 158, 'fc2_size': 107}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.09 minutes for training, 0.08 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 4.56883289776037e-05, 'weight_decay': 6.736006853376184e-06, 'momentum': 0.9148585589403455, 'conv1_size': 70, 'conv2_size': 14, 'conv3_size': 44, 'conv4_size': 29, 'conv5_size': 19, 'conv6_size': 58, 'fc1_size': 194, 'fc2_size': 46}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.00082500587037637, 'weight_decay': 0.00017065844924492977, 'momentum': 0.5796226178158025, 'conv1_size': 93, 'conv2_size': 39, 'conv3_size': 36, 'conv4_size': 45, 'conv5_size': 79, 'conv6_size': 45, 'fc1_size': 60, 'fc2_size': 158}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0005621707847439029, 'weight_decay': 6.128014305173467e-05, 'momentum': 0.1937536950155189, 'conv1_size': 40, 'conv2_size': 12, 'conv3_size': 12, 'conv4_size': 62, 'conv5_size': 73, 'conv6_size': 95, 'fc1_size': 183, 'fc2_size': 160}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.03 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.03 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.03 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 9.256549089517539e-05, 'weight_decay': 0.007404857243098157, 'momentum': 0.5216374848519616, 'conv1_size': 53, 'conv2_size': 67, 'conv3_size': 97, 'conv4_size': 89, 'conv5_size': 40, 'conv6_size': 81, 'fc1_size': 117, 'fc2_size': 117}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.0621338876144172e-05, 'weight_decay': 2.7902885686539008e-06, 'momentum': 0.2310482264124956, 'conv1_size': 44, 'conv2_size': 56, 'conv3_size': 46, 'conv4_size': 80, 'conv5_size': 42, 'conv6_size': 99, 'fc1_size': 182, 'fc2_size': 174}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.04 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.32, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.05 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.011656979858760895, 'weight_decay': 0.006264634203237732, 'momentum': 0.5648314740116656, 'conv1_size': 80, 'conv2_size': 44, 'conv3_size': 76, 'conv4_size': 93, 'conv5_size': 41, 'conv6_size': 95, 'fc1_size': 152, 'fc2_size': 116}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.06 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.306 loss\n",
      "{'lr': 0.031835947535209984, 'weight_decay': 3.0883526358543083e-06, 'momentum': 0.25721615722368213, 'conv1_size': 95, 'conv2_size': 24, 'conv3_size': 37, 'conv4_size': 96, 'conv5_size': 93, 'conv6_size': 90, 'fc1_size': 177, 'fc2_size': 143}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.32, avg_val_loss=2.31, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.08 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.27, avg_val_loss=2.24, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.21, avg_val_loss=2.19, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.17, avg_val_loss=2.20, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.15, avg_val_loss=2.15, batches=199 \n",
      " Took 0.07 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.150 loss\n",
      "{'lr': 1.3346922446660334e-06, 'weight_decay': 3.4046005510111173e-06, 'momentum': 0.22106116713432822, 'conv1_size': 68, 'conv2_size': 36, 'conv3_size': 47, 'conv4_size': 50, 'conv5_size': 62, 'conv6_size': 87, 'fc1_size': 76, 'fc2_size': 132}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.05 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=199 \n",
      " Took 0.06 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "Training with subset 0.0400, which is 1600 images\n",
      "{'lr': 1.2665859208481391e-05, 'weight_decay': 0.0004597084306065574, 'momentum': 0.20157789662317338, 'conv1_size': 42, 'conv2_size': 94, 'conv3_size': 34, 'conv4_size': 22, 'conv5_size': 43, 'conv6_size': 57, 'fc1_size': 188, 'fc2_size': 141}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.00419988809414591, 'weight_decay': 1.693139359106321e-05, 'momentum': 0.12645538421219735, 'conv1_size': 14, 'conv2_size': 20, 'conv3_size': 66, 'conv4_size': 67, 'conv5_size': 51, 'conv6_size': 99, 'fc1_size': 64, 'fc2_size': 175}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.4176634492558418e-05, 'weight_decay': 2.437328750023123e-06, 'momentum': 0.12556881640181974, 'conv1_size': 38, 'conv2_size': 30, 'conv3_size': 89, 'conv4_size': 81, 'conv5_size': 63, 'conv6_size': 47, 'fc1_size': 189, 'fc2_size': 61}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.4857581935558874e-05, 'weight_decay': 0.00046185189034075, 'momentum': 0.6548332190380699, 'conv1_size': 76, 'conv2_size': 12, 'conv3_size': 93, 'conv4_size': 51, 'conv5_size': 64, 'conv6_size': 12, 'fc1_size': 143, 'fc2_size': 41}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.08 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.12 minutes for training, 0.08 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.08 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 1.2339481130700368e-05, 'weight_decay': 1.9508354850686257e-06, 'momentum': 0.337649279746724, 'conv1_size': 44, 'conv2_size': 70, 'conv3_size': 55, 'conv4_size': 69, 'conv5_size': 49, 'conv6_size': 45, 'fc1_size': 53, 'fc2_size': 103}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.002017868122775421, 'weight_decay': 1.4750465496037542e-05, 'momentum': 0.737432347523046, 'conv1_size': 87, 'conv2_size': 27, 'conv3_size': 72, 'conv4_size': 87, 'conv5_size': 58, 'conv6_size': 43, 'fc1_size': 97, 'fc2_size': 167}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.007343427965554356, 'weight_decay': 0.0012687623638725012, 'momentum': 0.15023058537376635, 'conv1_size': 43, 'conv2_size': 11, 'conv3_size': 26, 'conv4_size': 69, 'conv5_size': 12, 'conv6_size': 98, 'fc1_size': 91, 'fc2_size': 166}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.00012409026676697343, 'weight_decay': 0.002000704149294447, 'momentum': 0.8662555643606971, 'conv1_size': 27, 'conv2_size': 59, 'conv3_size': 65, 'conv4_size': 52, 'conv5_size': 84, 'conv6_size': 96, 'fc1_size': 36, 'fc2_size': 112}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.08 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.00725477137577578, 'weight_decay': 0.0008396193088696935, 'momentum': 0.11660574628889861, 'conv1_size': 18, 'conv2_size': 76, 'conv3_size': 22, 'conv4_size': 77, 'conv5_size': 46, 'conv6_size': 65, 'fc1_size': 138, 'fc2_size': 55}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.07 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 8.888554715190321e-05, 'weight_decay': 6.0856668875623484e-06, 'momentum': 0.3972302289201086, 'conv1_size': 81, 'conv2_size': 51, 'conv3_size': 14, 'conv4_size': 66, 'conv5_size': 46, 'conv6_size': 45, 'fc1_size': 91, 'fc2_size': 42}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.14 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 1.1448973420337458e-05, 'weight_decay': 1.1990242020651353e-05, 'momentum': 0.11114672841149217, 'conv1_size': 79, 'conv2_size': 65, 'conv3_size': 70, 'conv4_size': 23, 'conv5_size': 79, 'conv6_size': 23, 'fc1_size': 85, 'fc2_size': 95}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.14 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.14 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.14 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.14 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.013798723586919522, 'weight_decay': 6.099825387453301e-05, 'momentum': 0.338947189757861, 'conv1_size': 59, 'conv2_size': 28, 'conv3_size': 97, 'conv4_size': 70, 'conv5_size': 84, 'conv6_size': 25, 'fc1_size': 79, 'fc2_size': 114}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.29, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.26, avg_val_loss=2.22, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.222 loss\n",
      "{'lr': 0.0003484692791380825, 'weight_decay': 0.0007457260711218003, 'momentum': 0.656574651977646, 'conv1_size': 96, 'conv2_size': 72, 'conv3_size': 51, 'conv4_size': 73, 'conv5_size': 43, 'conv6_size': 82, 'fc1_size': 142, 'fc2_size': 91}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.08 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 3.230196433207628e-06, 'weight_decay': 0.001992286347057884, 'momentum': 0.16818127520599324, 'conv1_size': 95, 'conv2_size': 90, 'conv3_size': 25, 'conv4_size': 54, 'conv5_size': 30, 'conv6_size': 14, 'fc1_size': 141, 'fc2_size': 179}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.08 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.17 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.18 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 8.34097291336242e-06, 'weight_decay': 1.8462537306916255e-05, 'momentum': 0.11881330680265191, 'conv1_size': 27, 'conv2_size': 27, 'conv3_size': 33, 'conv4_size': 34, 'conv5_size': 95, 'conv6_size': 76, 'fc1_size': 123, 'fc2_size': 100}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.07 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 3.1716771248393716e-06, 'weight_decay': 1.4109837559006178e-05, 'momentum': 0.5069086180371465, 'conv1_size': 62, 'conv2_size': 99, 'conv3_size': 85, 'conv4_size': 67, 'conv5_size': 70, 'conv6_size': 54, 'fc1_size': 189, 'fc2_size': 34}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.14 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.13 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.00019846335905869279, 'weight_decay': 0.0003077631269672481, 'momentum': 0.12202454068669076, 'conv1_size': 64, 'conv2_size': 22, 'conv3_size': 99, 'conv4_size': 75, 'conv5_size': 25, 'conv6_size': 28, 'fc1_size': 66, 'fc2_size': 111}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.09 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.05407274992097154, 'weight_decay': 1.0844482660909985e-05, 'momentum': 0.29945510081393556, 'conv1_size': 57, 'conv2_size': 64, 'conv3_size': 64, 'conv4_size': 87, 'conv5_size': 15, 'conv6_size': 93, 'fc1_size': 151, 'fc2_size': 71}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.27, batches=399 \n",
      " Took 0.11 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.18, avg_val_loss=2.10, batches=399 \n",
      " Took 0.12 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.09, avg_val_loss=2.02, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.05, avg_val_loss=2.00, batches=399 \n",
      " Took 0.11 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.03, avg_val_loss=2.02, batches=399 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.00, avg_val_loss=1.97, batches=399 \n",
      " Took 0.11 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=1.99, avg_val_loss=1.89, batches=399 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=1.95, avg_val_loss=1.89, batches=399 \n",
      " Took 0.10 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=1.92, avg_val_loss=1.99, batches=399 \n",
      " Took 0.10 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.891 loss\n",
      "{'lr': 8.715592310548871e-05, 'weight_decay': 3.057408277259039e-06, 'momentum': 0.43598329311784395, 'conv1_size': 59, 'conv2_size': 55, 'conv3_size': 19, 'conv4_size': 86, 'conv5_size': 74, 'conv6_size': 82, 'fc1_size': 39, 'fc2_size': 78}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.12 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.11 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.12 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.11 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.12 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 2.5060153025159325e-06, 'weight_decay': 0.00031805771560384615, 'momentum': 0.40825703514759426, 'conv1_size': 15, 'conv2_size': 45, 'conv3_size': 85, 'conv4_size': 63, 'conv5_size': 26, 'conv6_size': 26, 'fc1_size': 58, 'fc2_size': 71}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.05 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=399 \n",
      " Took 0.06 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=399 \n",
      " Took 0.07 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "Training with subset 0.0800, which is 3200 images\n",
      "{'lr': 7.316985092479574e-06, 'weight_decay': 0.007037857122622681, 'momentum': 0.5509904481180297, 'conv1_size': 30, 'conv2_size': 37, 'conv3_size': 79, 'conv4_size': 63, 'conv5_size': 58, 'conv6_size': 56, 'fc1_size': 163, 'fc2_size': 188}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.14 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.14 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.14 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.15 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.14 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.14 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.00012431878982079768, 'weight_decay': 1.6952961549014634e-06, 'momentum': 0.31689100392781155, 'conv1_size': 83, 'conv2_size': 43, 'conv3_size': 65, 'conv4_size': 23, 'conv5_size': 57, 'conv6_size': 97, 'fc1_size': 182, 'fc2_size': 70}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.2978805833160174e-05, 'weight_decay': 3.570637246399766e-05, 'momentum': 0.19340686897981832, 'conv1_size': 83, 'conv2_size': 47, 'conv3_size': 98, 'conv4_size': 21, 'conv5_size': 62, 'conv6_size': 41, 'fc1_size': 35, 'fc2_size': 110}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.27 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.305 loss\n",
      "{'lr': 0.0015370325671525962, 'weight_decay': 2.181291560060799e-06, 'momentum': 0.11928747046242948, 'conv1_size': 78, 'conv2_size': 35, 'conv3_size': 10, 'conv4_size': 54, 'conv5_size': 40, 'conv6_size': 18, 'fc1_size': 34, 'fc2_size': 129}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.2121609770158582e-06, 'weight_decay': 1.2484803670437954e-06, 'momentum': 0.12385713133887558, 'conv1_size': 51, 'conv2_size': 34, 'conv3_size': 78, 'conv4_size': 44, 'conv5_size': 32, 'conv6_size': 41, 'fc1_size': 117, 'fc2_size': 104}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.18 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.005377315853841156, 'weight_decay': 0.0026238150954443597, 'momentum': 0.2866157169893242, 'conv1_size': 51, 'conv2_size': 29, 'conv3_size': 12, 'conv4_size': 99, 'conv5_size': 49, 'conv6_size': 47, 'fc1_size': 32, 'fc2_size': 154}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.18 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 3.342568185967132e-06, 'weight_decay': 1.0712684655167535e-06, 'momentum': 0.9610941390456228, 'conv1_size': 88, 'conv2_size': 51, 'conv3_size': 85, 'conv4_size': 13, 'conv5_size': 16, 'conv6_size': 94, 'fc1_size': 108, 'fc2_size': 110}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.002517566683249272, 'weight_decay': 1.9322823705919495e-05, 'momentum': 0.14394226209718247, 'conv1_size': 46, 'conv2_size': 81, 'conv3_size': 75, 'conv4_size': 42, 'conv5_size': 99, 'conv6_size': 43, 'fc1_size': 166, 'fc2_size': 147}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.20 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 3.440669480136769e-06, 'weight_decay': 0.006461090695457354, 'momentum': 0.5532144657677441, 'conv1_size': 53, 'conv2_size': 48, 'conv3_size': 85, 'conv4_size': 44, 'conv5_size': 60, 'conv6_size': 90, 'fc1_size': 95, 'fc2_size': 32}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.32, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.20 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.19 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.20 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.306 loss\n",
      "{'lr': 0.004125015171997911, 'weight_decay': 5.405977764201998e-05, 'momentum': 0.6472821803281894, 'conv1_size': 10, 'conv2_size': 66, 'conv3_size': 60, 'conv4_size': 34, 'conv5_size': 93, 'conv6_size': 23, 'fc1_size': 199, 'fc2_size': 129}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.13 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.12 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.12 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.12 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.12 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.14 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.12 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.13 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.29, avg_val_loss=2.26, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.17, avg_val_loss=2.09, batches=799 \n",
      " Took 0.11 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.089 loss\n",
      "{'lr': 0.002167832499367348, 'weight_decay': 0.00021831134112789785, 'momentum': 0.41988907099139183, 'conv1_size': 81, 'conv2_size': 34, 'conv3_size': 24, 'conv4_size': 77, 'conv5_size': 75, 'conv6_size': 60, 'fc1_size': 147, 'fc2_size': 54}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.9368858938566267e-05, 'weight_decay': 0.0004047342854449137, 'momentum': 0.3392603213182844, 'conv1_size': 73, 'conv2_size': 51, 'conv3_size': 98, 'conv4_size': 52, 'conv5_size': 16, 'conv6_size': 27, 'fc1_size': 153, 'fc2_size': 91}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.24 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 2.3490166431666315e-06, 'weight_decay': 0.0010387424416618348, 'momentum': 0.2251001972769253, 'conv1_size': 69, 'conv2_size': 56, 'conv3_size': 27, 'conv4_size': 52, 'conv5_size': 99, 'conv6_size': 49, 'fc1_size': 136, 'fc2_size': 164}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.25 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0018176636233172496, 'weight_decay': 0.0014818975268884576, 'momentum': 0.35941583209988376, 'conv1_size': 27, 'conv2_size': 86, 'conv3_size': 33, 'conv4_size': 41, 'conv5_size': 27, 'conv6_size': 20, 'fc1_size': 150, 'fc2_size': 125}\n",
      "Inside training, model ID 4804167496\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.15 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.15 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.15 minutes for training, 0.03 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.15 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.15 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.8906266998831849e-06, 'weight_decay': 0.004595275158886408, 'momentum': 0.531801120434903, 'conv1_size': 42, 'conv2_size': 40, 'conv3_size': 23, 'conv4_size': 89, 'conv5_size': 88, 'conv6_size': 88, 'fc1_size': 198, 'fc2_size': 157}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.18 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.16 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 2.3539143940932777e-06, 'weight_decay': 1.3007396623325262e-05, 'momentum': 0.15034993189174808, 'conv1_size': 81, 'conv2_size': 62, 'conv3_size': 35, 'conv4_size': 96, 'conv5_size': 34, 'conv6_size': 82, 'fc1_size': 146, 'fc2_size': 61}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.009169062146694542, 'weight_decay': 1.241878249193543e-06, 'momentum': 0.30291413586018506, 'conv1_size': 49, 'conv2_size': 92, 'conv3_size': 79, 'conv4_size': 42, 'conv5_size': 20, 'conv6_size': 33, 'fc1_size': 172, 'fc2_size': 108}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=799 \n",
      " Took 0.21 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.27, avg_val_loss=2.19, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.13, avg_val_loss=2.04, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.04, avg_val_loss=1.95, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=1.98, avg_val_loss=1.92, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=1.94, avg_val_loss=1.86, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=1.91, avg_val_loss=1.85, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=1.88, avg_val_loss=1.84, batches=799 \n",
      " Took 0.21 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 1.843 loss\n",
      "{'lr': 3.796496304689716e-05, 'weight_decay': 2.5354673854947405e-05, 'momentum': 0.13200471394643487, 'conv1_size': 69, 'conv2_size': 87, 'conv3_size': 70, 'conv4_size': 62, 'conv5_size': 93, 'conv6_size': 44, 'fc1_size': 36, 'fc2_size': 198}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.28 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.26 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.27 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 8.221642776580729e-06, 'weight_decay': 2.166086886021847e-05, 'momentum': 0.10998739635914426, 'conv1_size': 63, 'conv2_size': 14, 'conv3_size': 56, 'conv4_size': 15, 'conv5_size': 60, 'conv6_size': 35, 'fc1_size': 136, 'fc2_size': 172}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.18 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.17 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.0003705213570133826, 'weight_decay': 2.1981602750340533e-05, 'momentum': 0.46870438425564, 'conv1_size': 66, 'conv2_size': 29, 'conv3_size': 70, 'conv4_size': 95, 'conv5_size': 21, 'conv6_size': 73, 'fc1_size': 96, 'fc2_size': 189}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=799 \n",
      " Took 0.23 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.23 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=799 \n",
      " Took 0.22 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "Training with subset 0.1600, which is 6400 images\n",
      "{'lr': 0.0004842906767770401, 'weight_decay': 1.0543402774914446e-06, 'momentum': 0.12163879366835097, 'conv1_size': 51, 'conv2_size': 43, 'conv3_size': 14, 'conv4_size': 23, 'conv5_size': 52, 'conv6_size': 84, 'fc1_size': 95, 'fc2_size': 59}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.36 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.34 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 3.720621717684943e-05, 'weight_decay': 2.4242635988332747e-06, 'momentum': 0.35370772349524177, 'conv1_size': 26, 'conv2_size': 71, 'conv3_size': 98, 'conv4_size': 94, 'conv5_size': 15, 'conv6_size': 41, 'fc1_size': 183, 'fc2_size': 78}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.31 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.31 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.31 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.30 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 5.4269997427324767e-05, 'weight_decay': 0.0009406575572479949, 'momentum': 0.20796681754556742, 'conv1_size': 49, 'conv2_size': 49, 'conv3_size': 90, 'conv4_size': 74, 'conv5_size': 64, 'conv6_size': 88, 'fc1_size': 50, 'fc2_size': 177}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.38 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.38 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.37 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.030221890529444e-06, 'weight_decay': 5.835803749520943e-06, 'momentum': 0.8266140690360734, 'conv1_size': 93, 'conv2_size': 70, 'conv3_size': 46, 'conv4_size': 50, 'conv5_size': 64, 'conv6_size': 17, 'fc1_size': 96, 'fc2_size': 151}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 2.49601008130502e-05, 'weight_decay': 0.0034214026161344282, 'momentum': 0.4663625733215825, 'conv1_size': 22, 'conv2_size': 10, 'conv3_size': 21, 'conv4_size': 79, 'conv5_size': 52, 'conv6_size': 22, 'fc1_size': 95, 'fc2_size': 87}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.00593678978297848, 'weight_decay': 2.0353649463033654e-06, 'momentum': 0.10963478198051073, 'conv1_size': 72, 'conv2_size': 70, 'conv3_size': 18, 'conv4_size': 18, 'conv5_size': 73, 'conv6_size': 29, 'fc1_size': 33, 'fc2_size': 113}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.49 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.48 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.48 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.49 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.49 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.29, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.22, avg_val_loss=2.08, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.02, avg_val_loss=1.96, batches=1599 \n",
      " Took 0.47 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=1.93, avg_val_loss=1.95, batches=1599 \n",
      " Took 0.48 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 1.948 loss\n",
      "{'lr': 3.855150981489978e-05, 'weight_decay': 1.4710370832320246e-05, 'momentum': 0.1718706270063546, 'conv1_size': 85, 'conv2_size': 68, 'conv3_size': 88, 'conv4_size': 60, 'conv5_size': 70, 'conv6_size': 86, 'fc1_size': 136, 'fc2_size': 44}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.304 loss\n",
      "{'lr': 0.028484817385563094, 'weight_decay': 9.767476675493436e-06, 'momentum': 0.5746984506920289, 'conv1_size': 16, 'conv2_size': 65, 'conv3_size': 97, 'conv4_size': 24, 'conv5_size': 25, 'conv6_size': 45, 'fc1_size': 164, 'fc2_size': 31}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.25 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.26 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.24 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.25 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.306 loss\n",
      "{'lr': 8.700563875988773e-05, 'weight_decay': 1.8725927236501513e-05, 'momentum': 0.28954626067207784, 'conv1_size': 43, 'conv2_size': 74, 'conv3_size': 82, 'conv4_size': 97, 'conv5_size': 88, 'conv6_size': 31, 'fc1_size': 85, 'fc2_size': 148}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.38 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.38 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.38 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 5.700573258757181e-06, 'weight_decay': 6.729785715936655e-05, 'momentum': 0.15643608253150634, 'conv1_size': 92, 'conv2_size': 43, 'conv3_size': 48, 'conv4_size': 44, 'conv5_size': 28, 'conv6_size': 44, 'fc1_size': 93, 'fc2_size': 96}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.54 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.57 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.58 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.57 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 2.0930812264471235e-06, 'weight_decay': 0.00015280047952772366, 'momentum': 0.277863984318109, 'conv1_size': 16, 'conv2_size': 59, 'conv3_size': 25, 'conv4_size': 27, 'conv5_size': 95, 'conv6_size': 48, 'fc1_size': 129, 'fc2_size': 85}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.22 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.22 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.23 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.22 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.007054153338575125, 'weight_decay': 1.3224161171048246e-05, 'momentum': 0.12556485572897813, 'conv1_size': 90, 'conv2_size': 77, 'conv3_size': 68, 'conv4_size': 29, 'conv5_size': 62, 'conv6_size': 68, 'fc1_size': 80, 'fc2_size': 73}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.29, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.26, avg_val_loss=2.19, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.11, avg_val_loss=1.99, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=1.96, avg_val_loss=1.93, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=1.89, avg_val_loss=1.91, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=1.83, avg_val_loss=1.89, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=1.77, avg_val_loss=1.90, batches=1599 \n",
      " Took 0.63 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=1.70, avg_val_loss=1.79, batches=1599 \n",
      " Took 0.62 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 1.794 loss\n",
      "{'lr': 0.0016013737055147893, 'weight_decay': 0.00012065216115957482, 'momentum': 0.2531582463163805, 'conv1_size': 65, 'conv2_size': 86, 'conv3_size': 74, 'conv4_size': 95, 'conv5_size': 55, 'conv6_size': 13, 'fc1_size': 108, 'fc2_size': 190}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.52 minutes for training, 0.05 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.52 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.52 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.51 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 7.948868690316704e-06, 'weight_decay': 2.6157212614034055e-05, 'momentum': 0.4161872061558456, 'conv1_size': 42, 'conv2_size': 98, 'conv3_size': 95, 'conv4_size': 13, 'conv5_size': 19, 'conv6_size': 40, 'fc1_size': 98, 'fc2_size': 102}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.41 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.41 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.41 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.39 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.40 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 2.3153633904958923e-05, 'weight_decay': 1.2290135592012505e-05, 'momentum': 0.15133987384663555, 'conv1_size': 56, 'conv2_size': 93, 'conv3_size': 79, 'conv4_size': 70, 'conv5_size': 97, 'conv6_size': 54, 'fc1_size': 52, 'fc2_size': 45}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.47 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.49 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.305 loss\n",
      "{'lr': 1.9829604451721013e-06, 'weight_decay': 0.00046212773798843924, 'momentum': 0.8007451634460135, 'conv1_size': 68, 'conv2_size': 87, 'conv3_size': 29, 'conv4_size': 87, 'conv5_size': 59, 'conv6_size': 39, 'fc1_size': 125, 'fc2_size': 141}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.57 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.55 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.61 minutes for training, 0.09 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.59 minutes for training, 0.08 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.59 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.56 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.58 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0011105085371032784, 'weight_decay': 8.868053096385972e-05, 'momentum': 0.19836397458204594, 'conv1_size': 80, 'conv2_size': 28, 'conv3_size': 51, 'conv4_size': 24, 'conv5_size': 16, 'conv6_size': 56, 'fc1_size': 58, 'fc2_size': 96}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.48 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.44 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.44 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.46 minutes for training, 0.05 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.44 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.44 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 1.6731799724164115e-05, 'weight_decay': 8.361734481445148e-06, 'momentum': 0.13424434300553797, 'conv1_size': 37, 'conv2_size': 57, 'conv3_size': 75, 'conv4_size': 47, 'conv5_size': 54, 'conv6_size': 10, 'fc1_size': 44, 'fc2_size': 159}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.36 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.36 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.36 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.36 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.35 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 2.0841887626983463e-05, 'weight_decay': 1.3012618979492604e-06, 'momentum': 0.33099943164028567, 'conv1_size': 91, 'conv2_size': 70, 'conv3_size': 84, 'conv4_size': 87, 'conv5_size': 78, 'conv6_size': 97, 'fc1_size': 80, 'fc2_size': 166}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.66 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.66 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.64 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.65 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 5.3502870443908204e-06, 'weight_decay': 0.002163197174890006, 'momentum': 0.657187997351687, 'conv1_size': 77, 'conv2_size': 37, 'conv3_size': 22, 'conv4_size': 71, 'conv5_size': 54, 'conv6_size': 31, 'fc1_size': 98, 'fc2_size': 89}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=1599 \n",
      " Took 0.45 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.47 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.45 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.46 minutes for training, 0.06 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=1599 \n",
      " Took 0.44 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "Training with subset 0.3200, which is 12800 images\n",
      "{'lr': 0.0011849035264162313, 'weight_decay': 5.117566393585901e-06, 'momentum': 0.14401808083326523, 'conv1_size': 54, 'conv2_size': 10, 'conv3_size': 79, 'conv4_size': 87, 'conv5_size': 32, 'conv6_size': 51, 'fc1_size': 118, 'fc2_size': 77}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.04542865026767264, 'weight_decay': 2.6201701306810624e-05, 'momentum': 0.9765124213745875, 'conv1_size': 14, 'conv2_size': 42, 'conv3_size': 14, 'conv4_size': 70, 'conv5_size': 93, 'conv6_size': 97, 'fc1_size': 189, 'fc2_size': 128}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.42, avg_val_loss=2.40, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.42, avg_val_loss=2.42, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.42, avg_val_loss=2.42, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.42, avg_val_loss=2.42, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.42, avg_val_loss=2.40, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.42, avg_val_loss=2.41, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.42, avg_val_loss=2.42, batches=3199 \n",
      " Took 0.43 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.42, avg_val_loss=2.40, batches=3199 \n",
      " Took 0.44 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.398 loss\n",
      "{'lr': 0.005191499409888433, 'weight_decay': 2.3306559305988256e-05, 'momentum': 0.21260750054998734, 'conv1_size': 71, 'conv2_size': 49, 'conv3_size': 22, 'conv4_size': 16, 'conv5_size': 91, 'conv6_size': 82, 'fc1_size': 101, 'fc2_size': 169}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.92 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.94 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.95 minutes for training, 0.05 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.93 minutes for training, 0.05 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.94 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.93 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.29, batches=3199 \n",
      " Took 0.93 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.18, avg_val_loss=1.94, batches=3199 \n",
      " Took 0.93 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=1.92, avg_val_loss=1.83, batches=3199 \n",
      " Took 0.95 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=1.81, avg_val_loss=1.69, batches=3199 \n",
      " Took 0.94 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=1.70, avg_val_loss=1.64, batches=3199 \n",
      " Took 0.94 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=1.60, avg_val_loss=1.57, batches=3199 \n",
      " Took 0.93 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=1.50, avg_val_loss=1.53, batches=3199 \n",
      " Took 0.92 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=1.41, avg_val_loss=1.50, batches=3199 \n",
      " Took 0.92 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=1.33, avg_val_loss=1.36, batches=3199 \n",
      " Took 0.93 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 1.359 loss\n",
      "{'lr': 0.00015035301927223785, 'weight_decay': 5.600942763701935e-06, 'momentum': 0.3460170771268909, 'conv1_size': 46, 'conv2_size': 33, 'conv3_size': 67, 'conv4_size': 96, 'conv5_size': 37, 'conv6_size': 99, 'fc1_size': 174, 'fc2_size': 184}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.71 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.78 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.70 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0001990699865687804, 'weight_decay': 0.002400602507769526, 'momentum': 0.11254785207761417, 'conv1_size': 42, 'conv2_size': 77, 'conv3_size': 15, 'conv4_size': 74, 'conv5_size': 45, 'conv6_size': 24, 'fc1_size': 84, 'fc2_size': 161}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.81 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.71 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.73 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.72 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.71 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.73 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.73 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.71 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.72 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.72 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.71 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.72 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.73 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.72 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.72 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.00012455058699810886, 'weight_decay': 0.00034339908170674845, 'momentum': 0.13539658061970258, 'conv1_size': 31, 'conv2_size': 13, 'conv3_size': 25, 'conv4_size': 47, 'conv5_size': 34, 'conv6_size': 75, 'fc1_size': 188, 'fc2_size': 198}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.48 minutes for training, 0.03 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.04376920844846633, 'weight_decay': 1.6410945883520194e-06, 'momentum': 0.552591628288485, 'conv1_size': 43, 'conv2_size': 58, 'conv3_size': 48, 'conv4_size': 91, 'conv5_size': 25, 'conv6_size': 54, 'fc1_size': 95, 'fc2_size': 37}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.70 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.70 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 2.673706447241409e-06, 'weight_decay': 0.004096221004209857, 'momentum': 0.2512004126011274, 'conv1_size': 74, 'conv2_size': 82, 'conv3_size': 21, 'conv4_size': 96, 'conv5_size': 17, 'conv6_size': 82, 'fc1_size': 156, 'fc2_size': 115}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 1.08 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.08 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.08 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.06 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.10 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.08 minutes for training, 0.06 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.08 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.08 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.07 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.07 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.07 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.07 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.07 minutes for training, 0.06 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.09 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.07 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.486457043734985e-06, 'weight_decay': 0.0006465397217460317, 'momentum': 0.8331479028987284, 'conv1_size': 47, 'conv2_size': 85, 'conv3_size': 88, 'conv4_size': 10, 'conv5_size': 24, 'conv6_size': 24, 'fc1_size': 189, 'fc2_size': 195}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.83 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.81 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.80 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.81 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.82 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.80 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.81 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.81 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.82 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.81 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 5.822201167712359e-06, 'weight_decay': 0.006244140410628526, 'momentum': 0.7620228305872258, 'conv1_size': 54, 'conv2_size': 15, 'conv3_size': 43, 'conv4_size': 91, 'conv5_size': 57, 'conv6_size': 46, 'fc1_size': 35, 'fc2_size': 133}\n",
      "Inside training, model ID 4804167440\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.69 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.68 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.0002238221068153361, 'weight_decay': 0.00011758138396167675, 'momentum': 0.15812367518126788, 'conv1_size': 93, 'conv2_size': 91, 'conv3_size': 69, 'conv4_size': 31, 'conv5_size': 61, 'conv6_size': 32, 'fc1_size': 118, 'fc2_size': 125}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.07 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.07 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.43 minutes for training, 0.07 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.40 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.40 minutes for training, 0.07 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.42 minutes for training, 0.07 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.07 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.07 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.42 minutes for training, 0.07 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.41 minutes for training, 0.07 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.43 minutes for training, 0.07 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.43 minutes for training, 0.07 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.43 minutes for training, 0.07 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.03881652793302662, 'weight_decay': 1.0913322143048043e-05, 'momentum': 0.12124802670056911, 'conv1_size': 40, 'conv2_size': 54, 'conv3_size': 21, 'conv4_size': 66, 'conv5_size': 67, 'conv6_size': 21, 'fc1_size': 181, 'fc2_size': 131}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.67 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.65 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.007889975021177493, 'weight_decay': 2.518311441903969e-06, 'momentum': 0.40606875695136324, 'conv1_size': 19, 'conv2_size': 86, 'conv3_size': 67, 'conv4_size': 73, 'conv5_size': 32, 'conv6_size': 77, 'fc1_size': 125, 'fc2_size': 66}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.56 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.56 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.29, avg_val_loss=2.11, batches=3199 \n",
      " Took 0.55 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=1.97, avg_val_loss=1.83, batches=3199 \n",
      " Took 0.57 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.79, avg_val_loss=1.71, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=1.67, avg_val_loss=1.57, batches=3199 \n",
      " Took 0.56 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=1.55, avg_val_loss=1.47, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 8 done, avg_train_loss=1.44, avg_val_loss=1.44, batches=3199 \n",
      " Took 0.56 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=1.35, avg_val_loss=1.43, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=1.25, avg_val_loss=1.39, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=1.16, avg_val_loss=1.47, batches=3199 \n",
      " Took 0.56 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=1.06, avg_val_loss=1.38, batches=3199 \n",
      " Took 0.54 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=0.99, avg_val_loss=1.35, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=0.92, avg_val_loss=1.42, batches=3199 \n",
      " Took 0.55 minutes for training, 0.03 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.354 loss\n",
      "{'lr': 0.00018293057966406229, 'weight_decay': 0.004290902556092978, 'momentum': 0.33273084458812585, 'conv1_size': 45, 'conv2_size': 22, 'conv3_size': 64, 'conv4_size': 50, 'conv5_size': 28, 'conv6_size': 37, 'fc1_size': 43, 'fc2_size': 119}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.61 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.61 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.61 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.62 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.60 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.62 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.0021009209074655125, 'weight_decay': 0.00019528617412398715, 'momentum': 0.7406385432054915, 'conv1_size': 73, 'conv2_size': 92, 'conv3_size': 25, 'conv4_size': 71, 'conv5_size': 26, 'conv6_size': 98, 'fc1_size': 111, 'fc2_size': 86}\n",
      "Inside training, model ID 4804169288\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.14 minutes for training, 0.07 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.27, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.06, avg_val_loss=1.90, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.94, avg_val_loss=1.89, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=1.89, avg_val_loss=1.83, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=1.84, avg_val_loss=1.83, batches=3199 \n",
      " Took 1.10 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=1.73, avg_val_loss=1.69, batches=3199 \n",
      " Took 1.14 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=1.62, avg_val_loss=1.56, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=1.49, avg_val_loss=1.49, batches=3199 \n",
      " Took 1.13 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=1.39, avg_val_loss=1.43, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=1.30, avg_val_loss=1.39, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=1.21, avg_val_loss=1.39, batches=3199 \n",
      " Took 1.10 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=1.13, avg_val_loss=1.37, batches=3199 \n",
      " Took 1.11 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 1.374 loss\n",
      "{'lr': 0.00035658980977803996, 'weight_decay': 2.8596298201675584e-05, 'momentum': 0.46100705266092984, 'conv1_size': 40, 'conv2_size': 43, 'conv3_size': 85, 'conv4_size': 64, 'conv5_size': 29, 'conv6_size': 58, 'fc1_size': 76, 'fc2_size': 65}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.65 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.66 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.63 minutes for training, 0.04 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.64 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 4.70073007711993e-05, 'weight_decay': 0.00019687907740067868, 'momentum': 0.13064746837999078, 'conv1_size': 13, 'conv2_size': 95, 'conv3_size': 67, 'conv4_size': 92, 'conv5_size': 11, 'conv6_size': 70, 'fc1_size': 171, 'fc2_size': 34}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.51 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.52 minutes for training, 0.03 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.51 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.51 minutes for training, 0.03 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.49 minutes for training, 0.03 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.52 minutes for training, 0.03 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.51 minutes for training, 0.03 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.51 minutes for training, 0.03 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.50 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 0.0004847982343572891, 'weight_decay': 0.0003373020249900084, 'momentum': 0.28807917112665654, 'conv1_size': 81, 'conv2_size': 67, 'conv3_size': 17, 'conv4_size': 34, 'conv5_size': 51, 'conv6_size': 76, 'fc1_size': 193, 'fc2_size': 85}\n",
      "Inside training, model ID 4804169512\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.14 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.28 minutes for training, 0.08 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.28 minutes for training, 0.07 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.18 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.15 minutes for training, 0.06 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.14 minutes for training, 0.06 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.18 minutes for training, 0.06 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.13 minutes for training, 0.06 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.12 minutes for training, 0.06 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.302 loss\n",
      "{'lr': 0.000675726958116938, 'weight_decay': 0.0006740610695904742, 'momentum': 0.1422643261046255, 'conv1_size': 77, 'conv2_size': 28, 'conv3_size': 66, 'conv4_size': 80, 'conv5_size': 94, 'conv6_size': 41, 'fc1_size': 172, 'fc2_size': 150}\n",
      "Inside training, model ID 4804168280\n",
      "Epoch 0 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.88 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.91 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.90 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.91 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.90 minutes for training, 0.05 for eval.\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.90 minutes for training, 0.05 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.92 minutes for training, 0.06 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 1.00 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.91 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.90 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.92 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.91 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.92 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.91 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=3199 \n",
      " Took 0.92 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.301 loss\n",
      "{'lr': 0.010126683177338835, 'weight_decay': 2.940396876823535e-06, 'momentum': 0.8219702058328363, 'conv1_size': 48, 'conv2_size': 93, 'conv3_size': 15, 'conv4_size': 91, 'conv5_size': 53, 'conv6_size': 83, 'fc1_size': 197, 'fc2_size': 161}\n",
      "Inside training, model ID 4804167272\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=3199 \n",
      " Took 0.85 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.11, avg_val_loss=1.91, batches=3199 \n",
      " Took 0.84 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=1.86, avg_val_loss=1.77, batches=3199 \n",
      " Took 0.85 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 3 done, avg_train_loss=1.73, avg_val_loss=1.75, batches=3199 \n",
      " Took 0.84 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 4 done, avg_train_loss=1.66, avg_val_loss=1.66, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=1.62, avg_val_loss=1.65, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 6 done, avg_train_loss=1.58, avg_val_loss=1.61, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Found better model\n",
      "Epoch 7 done, avg_train_loss=1.54, avg_val_loss=1.76, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Epoch 8 done, avg_train_loss=1.57, avg_val_loss=1.63, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Epoch 9 done, avg_train_loss=1.56, avg_val_loss=1.74, batches=3199 \n",
      " Took 0.84 minutes for training, 0.05 for eval.\n",
      "Epoch 10 done, avg_train_loss=1.56, avg_val_loss=1.67, batches=3199 \n",
      " Took 0.84 minutes for training, 0.05 for eval.\n",
      "Epoch 11 done, avg_train_loss=1.63, avg_val_loss=1.70, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Epoch 12 done, avg_train_loss=1.64, avg_val_loss=1.69, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Epoch 13 done, avg_train_loss=1.64, avg_val_loss=2.00, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Epoch 14 done, avg_train_loss=1.68, avg_val_loss=1.80, batches=3199 \n",
      " Took 0.83 minutes for training, 0.05 for eval.\n",
      "Finished Training\n",
      "Returning net with 1.612 loss\n",
      "Training with subset 0.6400, which is 25600 images\n",
      "{'lr': 2.6767035029542438e-05, 'weight_decay': 3.8139897586476437e-06, 'momentum': 0.5098721263680925, 'conv1_size': 33, 'conv2_size': 56, 'conv3_size': 50, 'conv4_size': 36, 'conv5_size': 30, 'conv6_size': 60, 'fc1_size': 178, 'fc2_size': 68}\n",
      "Inside training, model ID 4804168728\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.22 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.21 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=6399 \n",
      " Took 1.20 minutes for training, 0.04 for eval.\n",
      "Epoch 3 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.20 minutes for training, 0.04 for eval.\n",
      "Epoch 4 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.19 minutes for training, 0.03 for eval.\n",
      "Found better model\n",
      "Epoch 5 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.21 minutes for training, 0.04 for eval.\n",
      "Epoch 6 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.21 minutes for training, 0.04 for eval.\n",
      "Epoch 7 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.22 minutes for training, 0.04 for eval.\n",
      "Epoch 8 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.19 minutes for training, 0.04 for eval.\n",
      "Epoch 9 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.19 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 10 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.23 minutes for training, 0.10 for eval.\n",
      "Epoch 11 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 2.00 minutes for training, 0.04 for eval.\n",
      "Epoch 12 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.29 minutes for training, 0.04 for eval.\n",
      "Epoch 13 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.22 minutes for training, 0.04 for eval.\n",
      "Epoch 14 done, avg_train_loss=2.30, avg_val_loss=2.30, batches=6399 \n",
      " Took 1.21 minutes for training, 0.04 for eval.\n",
      "Finished Training\n",
      "Returning net with 2.303 loss\n",
      "{'lr': 1.9863526011481384e-05, 'weight_decay': 5.308885980298392e-06, 'momentum': 0.10701373269098174, 'conv1_size': 56, 'conv2_size': 54, 'conv3_size': 77, 'conv4_size': 35, 'conv5_size': 62, 'conv6_size': 42, 'fc1_size': 64, 'fc2_size': 48}\n",
      "Inside training, model ID 4795796672\n",
      "Epoch 0 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=6399 \n",
      " Took 1.62 minutes for training, 0.04 for eval.\n",
      "Found better model\n",
      "Epoch 1 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=6399 \n",
      " Took 1.57 minutes for training, 0.04 for eval.\n",
      "Epoch 2 done, avg_train_loss=2.31, avg_val_loss=2.31, batches=6399 \n",
      " Took 1.68 minutes for training, 0.07 for eval.\n",
      "Found better model\n"
     ]
    }
   ],
   "source": [
    "test_acc = {}\n",
    "val_acc = {}\n",
    "train_acc = {}\n",
    "test_loss = {}\n",
    "\n",
    "# Testing parameters\n",
    "n_searches = 2\n",
    "n_epochs = 1\n",
    "n_val = 10\n",
    "\n",
    "# Real parameters\n",
    "n_searches = 20\n",
    "n_epochs = 15\n",
    "n_val = 500\n",
    "\n",
    "\n",
    "for train_size in dataset_size:\n",
    "    print('Training with subset %1.4f, which is %d images'%(train_size, train_size*total_train))\n",
    "    \n",
    "    test_acc[train_size] = []\n",
    "    test_loss[train_size] = []\n",
    "    val_acc[train_size] = []\n",
    "    train_acc[train_size] = []\n",
    "    \n",
    "    for trial in range(n_searches):\n",
    "        \n",
    "        hyperparam_dict = deeper_random_hyperparamters()\n",
    "        print(hyperparam_dict)\n",
    "        \n",
    "        net = DeeperNet(hyperparam_dict)\n",
    "        \n",
    "        net, loss_list, val_list = train_model(net, trainset_loaders[train_size], valloader, n_val, n_epochs=n_epochs,\n",
    "                                              lr=hyperparam_dict['lr'], \n",
    "                                               momentum=hyperparam_dict['momentum'], \n",
    "                                               weight_decay=hyperparam_dict['weight_decay']\n",
    "                                              )\n",
    "\n",
    "        test_loss[train_size].append((hyperparam_dict, loss)) \n",
    "        val_acc[train_size].append((hyperparam_dict, val_list)) \n",
    "        train_acc[train_size].append((hyperparam_dict, loss_list))\n",
    "\n",
    "\n",
    "        torch.save(net, 'deeperNet_trainset_%d_images_trial%d_val_loss_%1.2f.model'%((train_size*total_train), trial, val_list[-1]))\n",
    "        torch.save(hyperparam_dict, 'deeperNet_trainset_%d_images_trial%d_val_loss_%1.2f.hparams'%((train_size*total_train), trial, val_list[-1]))\n",
    "        \n",
    "        del net # Attempt to free-up memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model deeperNet_trainset_200_images_trial11_val_loss_2.30.model\n",
      "Test UID is 4834564864\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Loaded model deeperNet_trainset_400_images_trial19_val_loss_2.30.model\n",
      "Test UID is 4786004208\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Loaded model deeperNet_trainset_800_images_trial0_val_loss_2.30.model\n",
      "Test UID is 4804167384\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Loaded model deeperNet_trainset_1600_images_trial0_val_loss_2.30.model\n",
      "Test UID is 4804167944\n",
      "Accuracy of the network on the 10000 test images: 10 %\n",
      "Loaded model deeperNet_trainset_3200_images_trial19_val_loss_1.77.model\n",
      "Test UID is 4791583856\n",
      "Accuracy of the network on the 10000 test images: 35 %\n",
      "Loaded model deeperNet_trainset_6400_images_trial11_val_loss_1.60.model\n",
      "Test UID is 4786004208\n",
      "Accuracy of the network on the 10000 test images: 38 %\n",
      "Loaded model deeperNet_trainset_12800_images_trial16_val_loss_1.29.model\n",
      "Test UID is 4834566040\n",
      "Accuracy of the network on the 10000 test images: 50 %\n",
      "Loaded model deeperNet_trainset_25600_images_trial4_val_loss_1.32.model\n",
      "Test UID is 4791586320\n",
      "Accuracy of the network on the 10000 test images: 58 %\n",
      "Loaded model deeperNet_trainset_40000_images_trial8_val_loss_1.17.model\n",
      "Test UID is 4804169008\n",
      "Accuracy of the network on the 10000 test images: 64 %\n"
     ]
    }
   ],
   "source": [
    "d_test_acc, d_test_loss = test_best_saved_model(dataset_size, regexp='deeperNet_trainset_%d_images*.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Variable(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 14, 14])\n",
      "torch.Size([4, 16, 5, 5])\n",
      "torch.Size([4, 32, 4, 4])\n",
      "torch.Size([4, 32, 3, 3])\n",
      "torch.Size([4, 64, 2, 2])\n",
      "torch.Size([4, 128, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0169  0.0857  0.0492 -0.0714 -0.1155 -0.0234  0.0357 -0.0483 -0.0799 -0.0246\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DeeperNet()\n",
    "net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = Variable(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 8\n",
       " 6\n",
       " 8\n",
       " 0\n",
       "[torch.LongTensor of size 4]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
